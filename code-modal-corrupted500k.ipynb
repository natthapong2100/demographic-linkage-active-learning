{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e9f0b3",
   "metadata": {},
   "source": [
    "# Passive Learning (Manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff94638",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da889431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birth-Death Record Linkage System\n",
      "==================================================\n",
      "Loading datasets...\n",
      "Birth records: 1297360 rows, 39 columns\n",
      "Death records: 1297360 rows, 38 columns\n",
      "\n",
      "Birth Records Columns:\n",
      "['ID', 'family', 'marriage', \"child's forname(s)\", \"child's surname\", 'birth day', 'birth month', 'birth year', 'address', 'sex', \"father's forename\", \"father's surname\", \"father's occupation\", \"mother's forename\", \"mother's maiden surname\", \"mother's occupation\", \"day of parents' marriage\", \"month of parents' marriage\", \"year of parents' marriage\", \"place of parent's marriage\", 'illegit', 'notes', 'Death', 'CHILD_IDENTITY', 'MOTHER_IDENTITY', 'FATHER_IDENTITY', 'DEATH_RECORD_IDENTITY', 'PARENT_MARRIAGE_RECORD_IDENTITY', 'FATHER_BIRTH_RECORD_IDENTITY', 'MOTHER_BIRTH_RECORD_IDENTITY', 'MARRIAGE_RECORD_IDENTITY1', 'MARRIAGE_RECORD_IDENTITY2', 'MARRIAGE_RECORD_IDENTITY3', 'MARRIAGE_RECORD_IDENTITY4', 'MARRIAGE_RECORD_IDENTITY5', 'MARRIAGE_RECORD_IDENTITY6', 'MARRIAGE_RECORD_IDENTITY7', 'MARRIAGE_RECORD_IDENTITY8', 'IMMIGRANT_GENERATION']\n",
      "\n",
      "Death Records Columns:\n",
      "['ID', 'forename(s) of deceased', 'surname of deceased', 'occupation', 'marital status', 'sex', 'name of spouse', \"spouse's occ\", 'day', 'month', 'year', 'address', 'age at death', \"father's forename\", \"father's surname\", \"father's occupation\", 'if father deceased', \"mother's forename\", \"mother's maiden surname\", \"mother's occupation\", 'if mother deceased', 'death code A', 'death code B', 'death code C', 'notes1', 'Birth', 'mar', 'DECEASED_IDENTITY', 'MOTHER_IDENTITY', 'FATHER_IDENTITY', 'SPOUSE_IDENTITY', 'BIRTH_RECORD_IDENTITY', 'PARENT_MARRIAGE_RECORD_IDENTITY', 'FATHER_BIRTH_RECORD_IDENTITY', 'MOTHER_BIRTH_RECORD_IDENTITY', 'SPOUSE_MARRIAGE_RECORD_IDENTITY', 'SPOUSE_BIRTH_RECORD_IDENTITY', 'IMMIGRANT_GENERATION']\n",
      "\n",
      "Missing values in Birth Records:\n",
      "ID                        0\n",
      "family                    0\n",
      "marriage              24294\n",
      "child's forname(s)      111\n",
      "child's surname          33\n",
      "birth day               626\n",
      "birth month               0\n",
      "birth year                0\n",
      "address               23938\n",
      "sex                       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Death Records:\n",
      "ID                              0\n",
      "forename(s) of deceased        73\n",
      "surname of deceased            21\n",
      "occupation                 485927\n",
      "marital status                  0\n",
      "sex                             0\n",
      "name of spouse             578180\n",
      "spouse's occ               734233\n",
      "day                           375\n",
      "month                         158\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Birth Records: (1297360, 13)\n",
      "Cleaned Death Records: (1297360, 13)\n"
     ]
    }
   ],
   "source": [
    "# Birth-Death Record Linkage Analysis\n",
    "# A comprehensive record linkage system for linking birth and death records\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "# from jellyfish import jaro_winkler, damerau_levenshtein_distance\n",
    "from jellyfish import jaro_similarity, jaro_winkler_similarity, levenshtein_distance, damerau_levenshtein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Birth-Death Record Linkage System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Data Preprocessing\n",
    "\n",
    "\n",
    "# %%\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# clean data (500k)\n",
    "birth_df = pd.read_csv('linkage data/data_500k/corrupted/birth_records.csv')\n",
    "death_df = pd.read_csv('linkage data/data_500k/corrupted/death_records.csv')\n",
    "\n",
    "print(f\"Birth records: {birth_df.shape[0]} rows, {birth_df.shape[1]} columns\")\n",
    "print(f\"Death records: {death_df.shape[0]} rows, {death_df.shape[1]} columns\")\n",
    "\n",
    "# %%\n",
    "# Examine the structure of both datasets\n",
    "print(\"\\nBirth Records Columns:\")\n",
    "print(birth_df.columns.tolist())\n",
    "print(f\"\\nDeath Records Columns:\")\n",
    "print(death_df.columns.tolist())\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in Birth Records:\")\n",
    "print(birth_df.isnull().sum().head(10))\n",
    "print(f\"\\nMissing values in Death Records:\")\n",
    "print(death_df.isnull().sum().head(10))\n",
    "\n",
    "# %%\n",
    "# Data preprocessing - select relevant columns for linkage\n",
    "birth_columns = [\n",
    "    'ID', 'child\\'s forname(s)', 'child\\'s surname', 'sex', \n",
    "    'father\\'s forename', 'father\\'s surname', 'father\\'s occupation',\n",
    "    'mother\\'s forename', 'mother\\'s maiden surname', 'mother\\'s occupation',\n",
    "    'birth year', 'address', 'Death'\n",
    "]\n",
    "\n",
    "death_columns = [\n",
    "    'ID', 'forename(s) of deceased', 'surname of deceased', 'sex',\n",
    "    'father\\'s forename', 'father\\'s surname', 'father\\'s occupation',\n",
    "    'mother\\'s forename', 'mother\\'s maiden surname', 'mother\\'s occupation',\n",
    "    'year', 'address', 'age at death'\n",
    "]\n",
    "\n",
    "# Create clean datasets\n",
    "birth_clean = birth_df[birth_columns].copy()\n",
    "death_clean = death_df[death_columns].copy()\n",
    "\n",
    "# Rename columns for consistency\n",
    "birth_clean.columns = ['birth_id', 'forename', 'surname', 'sex', \n",
    "                       'father_forename', 'father_surname', 'father_occupation',\n",
    "                       'mother_forename', 'mother_surname', 'mother_occupation',\n",
    "                       'birth_year', 'address', 'death_link']\n",
    "\n",
    "death_clean.columns = ['death_id', 'forename', 'surname', 'sex',\n",
    "                       'father_forename', 'father_surname', 'father_occupation', \n",
    "                       'mother_forename', 'mother_surname', 'mother_occupation',\n",
    "                       'death_year', 'address', 'age_at_death']\n",
    "\n",
    "# Fill missing values with empty strings for string columns\n",
    "string_cols = ['forename', 'surname', 'sex', 'father_forename', 'father_surname', \n",
    "               'father_occupation', 'mother_forename', 'mother_surname', \n",
    "               'mother_occupation', 'address']\n",
    "\n",
    "for col in string_cols:\n",
    "    birth_clean[col] = birth_clean[col].fillna('').astype(str)\n",
    "    death_clean[col] = death_clean[col].fillna('').astype(str)\n",
    "\n",
    "print(f\"\\nCleaned Birth Records: {birth_clean.shape}\")\n",
    "print(f\"Cleaned Death Records: {death_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4a7b5",
   "metadata": {},
   "source": [
    "### Ground-truth function\n",
    "- can adjust the `data_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f957ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling datasets to 1500000 rows each...\n",
      "========================================\n",
      "Original birth records: 1,297,360\n",
      "Original death records: 1,297,360\n",
      "Sampled birth records: 1,297,360\n",
      "Sampled death records: 1,297,360\n",
      "Total pairs to create: 1,683,142,969,600\n",
      "Sampling completed!\n",
      "\n",
      "\n",
      "Ground truth created: 1297360 record pairs\n",
      "Positive matches: 1297360\n",
      "Negative matches: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample both datasets to XX rows\n",
    "data_size = 1500000 # go with the max size = 1.3 M rows\n",
    "# data_size = 300000\n",
    "print(f\"Sampling datasets to {data_size} rows each...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Original sizes\n",
    "print(f\"Original birth records: {len(birth_clean):,}\")\n",
    "print(f\"Original death records: {len(death_clean):,}\")\n",
    "\n",
    "# Sample birth records to XX rows\n",
    "birth_sample_size = min(data_size, len(birth_clean))\n",
    "birth_sampled = birth_clean.sample(n=birth_sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Sample death records to XX rows  \n",
    "death_sample_size = min(data_size, len(death_clean))\n",
    "death_sampled = death_clean.sample(n=death_sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampled birth records: {len(birth_sampled):,}\")\n",
    "print(f\"Sampled death records: {len(death_sampled):,}\")\n",
    "print(f\"Total pairs to create: {len(birth_sampled) * len(death_sampled):,}\")\n",
    "\n",
    "# Use sampled data for ground truth\n",
    "birth_clean = birth_sampled\n",
    "death_clean = death_sampled\n",
    "\n",
    "print(\"Sampling completed!\")\n",
    "print()\n",
    "\n",
    "def create_efficient_ground_truth(birth_df, death_df):\n",
    "    \"\"\"\n",
    "    Most efficient approach: directly create ground truth from death_link column\n",
    "    Only creates records for actual linked pairs, O(n) complexity\n",
    "    \"\"\"\n",
    "    ground_truth = []\n",
    "    \n",
    "    # Create death_id lookup for validation\n",
    "    valid_death_ids = set(death_df['death_id'].values)\n",
    "    \n",
    "    for _, birth_row in birth_df.iterrows():\n",
    "        birth_id = birth_row['birth_id']\n",
    "        death_link = birth_row['death_link']\n",
    "        \n",
    "        # Only process records with valid death links\n",
    "        if pd.notna(death_link):\n",
    "            # Verify the death_id exists in death records\n",
    "            if death_link in valid_death_ids:\n",
    "                ground_truth.append({\n",
    "                    'birth_id': birth_id,\n",
    "                    'death_id': death_link,\n",
    "                    'label': 1  # This is a confirmed match\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(ground_truth)\n",
    "\n",
    "\n",
    "ground_truth_df = create_efficient_ground_truth(birth_clean, death_clean)\n",
    "\n",
    "print(f\"\\nGround truth created: {len(ground_truth_df)} record pairs\")\n",
    "print(f\"Positive matches: {ground_truth_df['label'].sum()}\")\n",
    "print(f\"Negative matches: {len(ground_truth_df) - ground_truth_df['label'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f8741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         birth_id  death_id  label\n",
      "0         1474005   1474005      1\n",
      "1         1832197   1832197      1\n",
      "2         1874508   1874508      1\n",
      "3         1588408   1588408      1\n",
      "4         2243616   2243616      1\n",
      "...           ...       ...    ...\n",
      "1297355   1897259   1897259      1\n",
      "1297356   2444811   2444811      1\n",
      "1297357   1939747   1939747      1\n",
      "1297358   1813214   1813214      1\n",
      "1297359   1918301   1918301      1\n",
      "\n",
      "[1297360 rows x 3 columns]\n",
      "         birth_id  death_id  label\n",
      "0         1474005   1474005      1\n",
      "1         1832197   1832197      1\n",
      "2         1874508   1874508      1\n",
      "3         1588408   1588408      1\n",
      "4         2243616   2243616      1\n",
      "...           ...       ...    ...\n",
      "1297355   1897259   1897259      1\n",
      "1297356   2444811   2444811      1\n",
      "1297357   1939747   1939747      1\n",
      "1297358   1813214   1813214      1\n",
      "1297359   1918301   1918301      1\n",
      "\n",
      "[1297360 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# ground_truth_df.loc[ground_truth_df['label'] == 1]\n",
    "print(ground_truth_df)\n",
    "print(ground_truth_df.loc[ground_truth_df['label'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6cec",
   "metadata": {},
   "source": [
    "## 2. Indexing (Blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaaa385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating blocks with threshold: 0.7\n",
      "Creating MinHash signatures for birth records...\n",
      "Creating MinHash signatures for death records...\n",
      "Inserting signatures into LSH index...\n"
     ]
    }
   ],
   "source": [
    "def create_minhash_signature(record, columns):\n",
    "    \"\"\"Create MinHash signature for a record\"\"\"\n",
    "    minhash = MinHash()\n",
    "    for col in columns:\n",
    "        data = str(record[col]).lower()\n",
    "        for token in data.split():\n",
    "            minhash.update(token.encode('utf8'))\n",
    "    return minhash\n",
    "\n",
    "def blocking_with_minhash(birth_df, death_df, threshold):\n",
    "    \"\"\"Create blocks using MinHash LSH\"\"\"\n",
    "    print(f\"Creating blocks with threshold: {threshold}\")\n",
    "    \n",
    "    # Columns to use for blocking\n",
    "    blocking_cols = ['forename', 'surname', 'father_surname', 'mother_surname']\n",
    "    \n",
    "    # Create MinHash signatures for all records\n",
    "    birth_signatures = {}\n",
    "    death_signatures = {}\n",
    "    \n",
    "    print(\"Creating MinHash signatures for birth records...\")\n",
    "    for idx, row in birth_df.iterrows():\n",
    "        birth_signatures[f\"birth_{row['birth_id']}\"] = create_minhash_signature(row, blocking_cols)\n",
    "    \n",
    "    print(\"Creating MinHash signatures for death records...\")\n",
    "    for idx, row in death_df.iterrows():\n",
    "        death_signatures[f\"death_{row['death_id']}\"] = create_minhash_signature(row, blocking_cols)\n",
    "    \n",
    "    # Create LSH index\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    \n",
    "    # Insert all signatures\n",
    "    print(\"Inserting signatures into LSH index...\")\n",
    "    for key, sig in birth_signatures.items():\n",
    "        lsh.insert(key, sig)\n",
    "    for key, sig in death_signatures.items():\n",
    "        lsh.insert(key, sig)\n",
    "    \n",
    "    # Generate candidate pairs and detailed blocking information\n",
    "    candidate_pairs = []\n",
    "    birth_blocks = {}  # Dictionary to store blocks for each birth record\n",
    "    \n",
    "    print(\"Generating candidate pairs...\")\n",
    "    for birth_key, birth_sig in birth_signatures.items():\n",
    "        similar_records = lsh.query(birth_sig)\n",
    "        birth_id = int(birth_key.split('_')[1])\n",
    "        \n",
    "        # Find death records in the same block\n",
    "        death_records_in_block = []\n",
    "        for similar_key in similar_records:\n",
    "            if similar_key.startswith('death_'):\n",
    "                death_id = int(similar_key.split('_')[1])\n",
    "                candidate_pairs.append((birth_id, death_id))\n",
    "                death_records_in_block.append(death_id)\n",
    "        \n",
    "        # Store block information (just for statistical purposes)\n",
    "        birth_blocks[birth_id] = {\n",
    "            'birth_id': birth_id,\n",
    "            'death_records': death_records_in_block,\n",
    "            'total_similar_records': len(similar_records),\n",
    "            'death_count': len(death_records_in_block)\n",
    "        }\n",
    "    \n",
    "    # Remove duplicates from candidate pairs\n",
    "    candidate_pairs = list(set(candidate_pairs))\n",
    "    \n",
    "    print(f\"Generated {len(candidate_pairs)} candidate pairs\") # for computation\n",
    "    print(f\"Generated {len(birth_blocks)} birth blocks\") # for illustration, exploration\n",
    "    \n",
    "    return candidate_pairs, birth_blocks\n",
    "\n",
    "# Generate candidate pairs through blocking\n",
    "candidate_pairs, birth_blocks = blocking_with_minhash(birth_clean, death_clean, threshold=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db422e8b",
   "metadata": {},
   "source": [
    "- checking the candidate pair that is same with the birth_blocks variable\n",
    "- the diff is only the data types\n",
    "- And, birth_blocks count the one that dont have the death (candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_candidate_pairs_by_ID(checking_id):\n",
    "    r = []\n",
    "    for i in range(len(candidate_pairs)):\n",
    "        if candidate_pairs[i][0] == checking_id:\n",
    "            r.append(candidate_pairs[i])\n",
    "    print(f\"\\nCandidate pairs for birth ID {checking_id}: {r}\")\n",
    "    print(f\"Total candidate pairs for birth ID {checking_id}: {len(r)}\")\n",
    "\n",
    "checking_candidate_pairs_by_ID(checking_id=1383518)\n",
    "checking_candidate_pairs_by_ID(checking_id=2532606)\n",
    "checking_candidate_pairs_by_ID(checking_id=1276997)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110d6ce",
   "metadata": {},
   "source": [
    "### Blocking Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a58e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed blocking results\n",
    "print(\"\\nDetailed Blocking Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show first 15 birth records and their associated death records\n",
    "print(\"Sample Blocks (First 15 Birth Records):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "birth_ids_sample = list(birth_blocks.keys())[:15]\n",
    "for i, birth_id in enumerate(birth_ids_sample):\n",
    "    block_info = birth_blocks[birth_id]\n",
    "    print(f\"Block {i+1}:\")\n",
    "    print(f\"  Birth ID: {birth_id}\")\n",
    "    print(f\"  Associated Death Records: {block_info['death_records']}\")\n",
    "    print(f\"  Number of Death Records: {block_info['death_count']}\")\n",
    "    # print(f\"  Total Similar Records: {block_info['total_similar_records']}\")\n",
    "    \n",
    "    # Show names for context\n",
    "    birth_record = birth_clean[birth_clean['birth_id'] == birth_id].iloc[0]\n",
    "    print(f\"  Birth Record: {birth_record['forename']} {birth_record['surname']}\")\n",
    "    \n",
    "    if block_info['death_records']:\n",
    "        print(f\"  Death Record(s):\")\n",
    "        for death_id in block_info['death_records']:\n",
    "            death_record = death_clean[death_clean['death_id'] == death_id]\n",
    "            if not death_record.empty:\n",
    "                death_record = death_record.iloc[0]\n",
    "                print(f\"    ID {death_id}: {death_record['forename']} {death_record['surname']}\")\n",
    "    else:\n",
    "        print(f\"  No death records in this block\")\n",
    "    print(\"---\")\n",
    "\n",
    "# %%\n",
    "# Statistical analysis of blocking results\n",
    "print(f\"\\nBlocking Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "death_counts = [block_info['death_count'] for block_info in birth_blocks.values()]\n",
    "birth_records_with_deaths = sum(1 for count in death_counts if count > 0)\n",
    "birth_records_without_deaths = sum(1 for count in death_counts if count == 0)\n",
    "\n",
    "print(f\"Total birth records: {len(birth_blocks)}\")\n",
    "print(f\"Birth records with death candidates: {birth_records_with_deaths}\")\n",
    "print(f\"Birth records without death candidates: {birth_records_without_deaths}\")\n",
    "print(f\"Average death candidates per birth record: {np.mean(death_counts):.2f}\")\n",
    "print(f\"Max death candidates for a single birth record: {max(death_counts)}\")\n",
    "\n",
    "# %%\n",
    "# Distribution of death record counts per birth record\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(death_counts, bins=range(max(death_counts)+2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Number of Death Records per Birth Record')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Death Candidates per Birth Record')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show birth records with most death candidates\n",
    "plt.subplot(1, 2, 2)\n",
    "top_candidates = sorted(birth_blocks.items(), key=lambda x: x[1]['death_count'], reverse=True)[:10]\n",
    "birth_ids = [str(item[0]) for item in top_candidates]\n",
    "death_counts_top = [item[1]['death_count'] for item in top_candidates]\n",
    "\n",
    "plt.bar(birth_ids, death_counts_top)\n",
    "plt.xlabel('Birth Record ID')\n",
    "plt.ylabel('Number of Death Candidates')\n",
    "plt.title('Top 10 Birth Records by Death Candidates')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Show birth records with multiple death candidates\n",
    "print(f\"\\nBirth Records with Multiple Death Candidates:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "multiple_candidates = {birth_id: info for birth_id, info in birth_blocks.items() \n",
    "                      if info['death_count'] > 1}\n",
    "\n",
    "if multiple_candidates:\n",
    "    print(f\"Found {len(multiple_candidates)} birth records with multiple death candidates:\")\n",
    "    \n",
    "    for birth_id, block_info in sorted(multiple_candidates.items(), \n",
    "                                      key=lambda x: x[1]['death_count'], reverse=True)[:10]:\n",
    "        birth_record = birth_clean[birth_clean['birth_id'] == birth_id].iloc[0]\n",
    "        print(f\"\\nBirth ID {birth_id}: {birth_record['forename']} {birth_record['surname']}\")\n",
    "        print(f\"  Death candidates ({block_info['death_count']}):\")\n",
    "        \n",
    "        for death_id in block_info['death_records']:\n",
    "            death_record = death_clean[death_clean['death_id'] == death_id]\n",
    "            if not death_record.empty:\n",
    "                death_record = death_record.iloc[0]\n",
    "                print(f\"    ID {death_id}: {death_record['forename']} {death_record['surname']} \"\n",
    "                      f\"(Age: {death_record['age_at_death']})\")\n",
    "else:\n",
    "    print(\"No birth records have multiple death candidates with this threshold.\")\n",
    "\n",
    "# %%\n",
    "# Summary table\n",
    "print(f\"\\nBlocking Summary Table:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "summary_data = []\n",
    "for count in range(max(death_counts) + 1):\n",
    "    freq = death_counts.count(count)\n",
    "    percentage = (freq / len(death_counts)) * 100\n",
    "    summary_data.append({\n",
    "        'Death_Candidates': count,\n",
    "        'Birth_Records': freq,\n",
    "        'Percentage': f\"{percentage:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e30ed",
   "metadata": {},
   "source": [
    "## 3. Comparing\n",
    "### Feature\n",
    "- from append()\n",
    "- these are the features in each columns\n",
    "1. jaro winkler\n",
    "2. damerau levenshtein\n",
    "3. jaccard similarity\n",
    "4. age consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fe25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Comparing (Feature Engineering)\n",
    "\n",
    "# %%\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Calculate Jaccard similarity between two strings\"\"\"\n",
    "    set1 = set(str1.lower().split())\n",
    "    set2 = set(str2.lower().split())\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 0\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def calculate_features(birth_record, death_record):\n",
    "    \"\"\"Calculate similarity features between two records\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # String comparison columns\n",
    "    string_cols = ['forename', 'surname', 'sex', 'father_forename', 'father_surname', \n",
    "                   'mother_forename', 'mother_surname', 'address']\n",
    "    \n",
    "    for col in string_cols:\n",
    "        birth_val = str(birth_record[col]).lower()\n",
    "        death_val = str(death_record[col]).lower()\n",
    "        \n",
    "        # Jaro-Winkler similarity\n",
    "        jw_sim = jaro_winkler_similarity(birth_val, death_val)\n",
    "        features.append(jw_sim)\n",
    "        # features.append({'col': col, 'jw_sim': jw_sim})\n",
    "        \n",
    "        # Damerau-Levenshtein distance (normalized)\n",
    "        dl_dist = damerau_levenshtein_distance(birth_val, death_val)\n",
    "        max_len = max(len(birth_val), len(death_val))\n",
    "        dl_sim = 1 - (dl_dist / max_len) if max_len > 0 else 1\n",
    "        features.append(dl_sim)\n",
    "        # features.append({'col': col, 'dl_sim': dl_sim})\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        jaccard_sim = jaccard_similarity(birth_val, death_val)\n",
    "        features.append(jaccard_sim)\n",
    "        # features.append({'col': col, 'jaccard_sim': jaccard_sim})\n",
    "\n",
    "    try:\n",
    "        # Convert to numeric, handling potential string inputs\n",
    "        birth_year = pd.to_numeric(birth_record['birth_year'], errors='coerce')\n",
    "        death_year = pd.to_numeric(death_record['death_year'], errors='coerce')  \n",
    "        age_at_death = pd.to_numeric(death_record['age_at_death'], errors='coerce')\n",
    "        \n",
    "        if pd.notna(birth_year) and pd.notna(death_year) and pd.notna(age_at_death):\n",
    "            expected_birth_year = death_year - age_at_death\n",
    "            age_diff = abs(birth_year - expected_birth_year)\n",
    "            age_consistency = 1 / (1 + age_diff)  # Exponential decay\n",
    "            # features.append({'col': 'age_consistency', 'value': age_consistency})\n",
    "            features.append(age_consistency)\n",
    "        else:\n",
    "            # features.append({'col': 'age_consistency', 'value': 0.5})\n",
    "            features.append(0.5)\n",
    "    except:\n",
    "        # Fallback if conversion fails\n",
    "        # features.append({'col': 'age_consistency', 'value': 0.5})\n",
    "        features.append(0.5)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create feature matrix\n",
    "print(\"Calculating features for candidate pairs...\")\n",
    "X = []\n",
    "y = []\n",
    "candidate_pairs_list = []\n",
    "\n",
    "# Create lookup dictionaries for faster access\n",
    "birth_lookup = {row['birth_id']: row for _, row in birth_clean.iterrows()} # death link\n",
    "death_lookup = {row['death_id']: row for _, row in death_clean.iterrows()}\n",
    "ground_truth_lookup = {(row['birth_id'], row['death_id']): row['label'] \n",
    "                      for _, row in ground_truth_df.iterrows()}\n",
    "\n",
    "for birth_id, death_id in candidate_pairs: # use candidate pairs only\n",
    "    if birth_id in birth_lookup and death_id in death_lookup:\n",
    "        birth_record = birth_lookup[birth_id]\n",
    "        death_record = death_lookup[death_id]\n",
    "        \n",
    "        features = calculate_features(birth_record, death_record)\n",
    "        X.append(features)\n",
    "        \n",
    "        # Get ground truth label\n",
    "        label = ground_truth_lookup.get((birth_id, death_id))\n",
    "        label = int(label) if label is not None else 0  # Default to 0 if not found\n",
    "        y.append(label)\n",
    "\n",
    "        # Track the candidate pair for this feature row\n",
    "        candidate_pairs_list.append((birth_id, death_id))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)  # Ensure labels are integers\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "# print(f\"Positive class ratio: {np.sum(y)/len(y):.4f}\")\n",
    "\n",
    "print(\"Saving features and labels...\")\n",
    "\n",
    "# Option 1: Save as NumPy arrays (recommended for X, y)\n",
    "np.save('X_features.npy', X)\n",
    "np.save('y_labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[0:20]\n",
    "X[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84529a",
   "metadata": {},
   "source": [
    "## 4. Classification\n",
    "### Cross Validation (5 folds)\n",
    "- with Metrics performance; Precision, Recall, F1\n",
    "- draw Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b04035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "X_train, X_test, y_train, y_test, pairs_train, pairs_test = train_test_split(\n",
    "    X, y, candidate_pairs_list, \n",
    "    test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "## randomly select 100 candidate pairs for testing *****\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training positive ratio: {np.sum(y_train)/len(y_train):.4f}\")\n",
    "print(f\"Test positive ratio: {np.sum(y_test)/len(y_test):.4f}\")\n",
    "\n",
    "# %%\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# %%\n",
    "# Cross-Validation Analysis (on training data only)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "print(\"\\nPerforming Cross-Validation Analysis (Training Data Only)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# %%\n",
    "# Check if we have enough samples for cross-validation\n",
    "if len(np.unique(y_train)) < 2:\n",
    "    print(\"Warning: Only one class present in the training data. Cannot perform classification.\")\n",
    "    print(\"This might indicate:\")\n",
    "    print(\"1. Threshold too high (try lowering it)\")\n",
    "    print(\"2. Ground truth issues\")\n",
    "    print(\"3. No actual matches in the candidate pairs\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "else:\n",
    "    # Define custom scorers\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'f1': make_scorer(f1_score, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # %%\n",
    "    # Perform 5-fold stratified cross-validation on training data only\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"5-Fold Stratified Cross-Validation Results (Training Data Only):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Initialize classifier for CV\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation on training data only\n",
    "    cv_results = cross_validate(\n",
    "        rf_classifier, X_train_scaled, y_train, \n",
    "        cv=cv, scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # %%\n",
    "    # Display cross-validation results\n",
    "    metrics = ['precision', 'recall', 'f1']\n",
    "    for metric in metrics:\n",
    "        test_scores = cv_results[f'test_{metric}'] # test_precision, test_recall, test_f1\n",
    "        train_scores = cv_results[f'train_{metric}'] # train_precision, train_recall, train_f1\n",
    "        \n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  Training:    {train_scores.mean():.4f} (+/- {train_scores.std() * 2:.4f})\")\n",
    "        print(f\"  Validation:  {test_scores.mean():.4f} (+/- {test_scores.std() * 2:.4f})\")\n",
    "        print(f\"  CV Folds:    {test_scores}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2829f",
   "metadata": {},
   "source": [
    "### Fit the model and do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96609bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Train Random Forest classifier on training set\n",
    "print(f\"\\n\\nTraining Final Model on Training Set:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# %%\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test_scaled)[:, 1] # select all rows, 1 col\n",
    "\n",
    "print(y_pred)\n",
    "print(\"-\"* 40)\n",
    "count_of_ones = (y_pred == 1).sum()\n",
    "count_of_zeros = (y_pred == 0).sum()\n",
    "print(\"COunt of 0: \", count_of_zeros)\n",
    "print(\"COunt of 1: \", count_of_ones)\n",
    "print(\"-\"* 40)\n",
    "print(y_pred_proba)\n",
    "print(\"Classification completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b29730",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "- with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Evaluation\n",
    "\n",
    "# %%\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# %%\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Match', 'Match'],\n",
    "            yticklabels=['No Match', 'Match'])\n",
    "plt.title('Confusion Matrix - Birth-Death Record Linkage')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Feature importance analysis\n",
    "feature_names = []\n",
    "string_cols = ['forename', 'surname', 'sex', 'father_forename', 'father_surname', \n",
    "               'mother_forename', 'mother_surname', 'address']\n",
    "\n",
    "for col in string_cols:\n",
    "    feature_names.extend([f'{col}_jw', f'{col}_dl', f'{col}_jaccard'])\n",
    "feature_names.append('age_consistency')\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# %%\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int) \n",
    "    # the cutoff point of pred_proba to know what probabilities should we say \"this is a match\"\n",
    "    if np.sum(y_pred_thresh) > 0:\n",
    "        precisions.append(precision_score(y_test, y_pred_thresh))\n",
    "        recalls.append(recall_score(y_test, y_pred_thresh))\n",
    "        f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "    else:\n",
    "        precisions.append(0)\n",
    "        recalls.append(0)\n",
    "        f1_scores.append(0)\n",
    "\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o')\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='s')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score', marker='^')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance vs Threshold')\n",
    "plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Summary statistics\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"5-Fold Stratified Cross-Validation Results:\")\n",
    "for metric in metrics:\n",
    "    test_scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.capitalize()}: {test_scores.mean():.4f} (+/- {test_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total record pairs evaluated: {len(candidate_pairs)}\")\n",
    "print(f\"Actual matches in test set: {np.sum(y_test)}\")\n",
    "print(f\"Predicted matches in test set: {np.sum(y_pred)}\")\n",
    "print(f\"True positives: {cm[1,1]}\")\n",
    "print(f\"False positives: {cm[0,1]}\")\n",
    "print(f\"False negatives: {cm[1,0]}\")\n",
    "print(f\"True negatives: {cm[0,0]}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Sample prediction results\n",
    "# Save results with correct candidate pairs alignment\n",
    "pairs_test_array = np.array(pairs_test)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'birth_id': pairs_test_array[:, 0],          \n",
    "    'death_id': pairs_test_array[:, 1],\n",
    "    'actual_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'prediction_probability': y_pred_proba\n",
    "})\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# view the inside data of FP that we missed\n",
    "false_positives = results_df[(results_df['actual_label'] == 0) & \n",
    "                            (results_df['predicted_label'] == 1)]\n",
    "\n",
    "print(f\"\\nTotal False Positives: {len(false_positives)}\")\n",
    "\n",
    "if len(false_positives) > 0:\n",
    "    fp_sorted = false_positives.sort_values('prediction_probability', ascending=False)\n",
    "    print(f\"Top 10 False Positives:\")\n",
    "    print(fp_sorted.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"ðŸŽ‰ No False Positives!\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# view the inside data of FN that we missed\n",
    "false_negative = results_df[(results_df['actual_label'] == 1) & \n",
    "                            (results_df['predicted_label'] == 0)]\n",
    "\n",
    "print(f\"\\nTotal False Negatives: {len(false_negative)}\")\n",
    "\n",
    "if len(false_negative) > 0:\n",
    "    fn_sorted = false_negative.sort_values('prediction_probability', ascending=False)\n",
    "    print(f\"Top 10 False Negatives:\")\n",
    "    print(fn_sorted.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"ðŸŽ‰ No False Negatives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1245b",
   "metadata": {},
   "source": [
    "# Active Learning (500k dataset version)\n",
    "- approx. 1.3 M rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63c21a",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Active Learning with modAL\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, entropy_sampling, margin_sampling\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Starting Active Learning with modAL...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# %%\n",
    "# Prepare data for active learning\n",
    "print(\"Preparing data for active learning...\")\n",
    "\n",
    "# Use the same train/test split but treat training data as \"unlabeled pool\"\n",
    "X_pool = X_train.copy()  # Pool of unlabeled data\n",
    "y_pool = y_train.copy()         # True labels (hidden from learner initially)\n",
    "X_test_al = X_test.copy()  # Test set remains the same\n",
    "y_test_al = y_test.copy()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Pool size: {len(X_pool)} samples\")\n",
    "print(f\"Test size: {len(X_test_al)} samples\")\n",
    "print(f\"Pool positive ratio: {np.sum(y_pool)/len(y_pool):.4f}\")\n",
    "\n",
    "# %%\n",
    "# Initialize with small labeled set\n",
    "initial_labeled_size = 10  # Start with only 10 labeled samples\n",
    "n_queries = 20             # Number of AL iterations\n",
    "\n",
    "# Randomly select initial labeled samples (stratified)\n",
    "initial_indices = []\n",
    "for class_label in [0, 1]:\n",
    "    class_indices = np.where(y_pool == class_label)[0] # find position of that particular class\n",
    "    n_samples = initial_labeled_size // 2  # 5 samples per class\n",
    "    selected = np.random.choice(class_indices, size=n_samples, replace=False) # select the position in random\n",
    "    initial_indices.extend(selected)\n",
    "\n",
    "initial_indices = np.array(initial_indices)\n",
    "\n",
    "# Split into initial labeled and remaining unlabeled\n",
    "X_initial = X_pool[initial_indices]\n",
    "y_initial = y_pool[initial_indices]\n",
    "\n",
    "# Remove initial samples from pool\n",
    "# give us the indices that are in the full range of X_pool BUT NOT in initial_indices\n",
    "remaining_indices = np.setdiff1d(range(len(X_pool)), initial_indices) \n",
    "X_unlabeled = X_pool[remaining_indices]\n",
    "y_unlabeled_true = y_pool[remaining_indices]  # True labels (hidden)\n",
    "\n",
    "print(f\"Initial labeled set: {len(X_initial)} samples\")\n",
    "print(f\"Remaining unlabeled pool: {len(X_unlabeled)} samples\")\n",
    "print(f\"Initial positive ratio: {np.sum(y_initial)/len(y_initial):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_initial[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf467e88",
   "metadata": {},
   "source": [
    "## Set Model and see initial result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Initialize Active Learner\n",
    "print(\"Initializing Active Learner...\")\n",
    "\n",
    "# Use Random Forest as base estimator\n",
    "rf_al = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Create Active Learner with uncertainty sampling\n",
    "learner = ActiveLearner(\n",
    "    estimator=rf_al,\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training=X_initial,\n",
    "    y_training=y_initial\n",
    ")\n",
    "\n",
    "print(\"Active Learner initialized!\")\n",
    "print(f\"Query strategy: uncertainty sampling\")\n",
    "\n",
    "# %%\n",
    "# Active Learning Loop\n",
    "print(f\"\\nStarting Active Learning Loop...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Track performance metrics\n",
    "al_results = {\n",
    "    'n_labeled': [],\n",
    "    'test_accuracy': [],\n",
    "    'test_precision': [],\n",
    "    'test_recall': [],\n",
    "    'test_f1': [],\n",
    "    'pool_size': []\n",
    "}\n",
    "\n",
    "# Evaluate initial performance\n",
    "y_pred_initial = learner.predict(X_test_al)\n",
    "\n",
    "initial_accuracy = accuracy_score(y_test_al, y_pred_initial)\n",
    "initial_precision = precision_score(y_test_al, y_pred_initial, zero_division=0)\n",
    "initial_recall = recall_score(y_test_al, y_pred_initial, zero_division=0)\n",
    "initial_f1 = f1_score(y_test_al, y_pred_initial, zero_division=0)\n",
    "\n",
    "print(f\"Initial performance (with {len(X_initial)} samples):\")\n",
    "print(f\"  Accuracy: {initial_accuracy:.4f}\")\n",
    "print(f\"  Precision: {initial_precision:.4f}\")\n",
    "print(f\"  Recall: {initial_recall:.4f}\")\n",
    "print(f\"  F1-Score: {initial_f1:.4f}\")\n",
    "\n",
    "# Store initial results\n",
    "al_results['n_labeled'].append(len(X_initial))\n",
    "al_results['test_accuracy'].append(initial_accuracy)\n",
    "al_results['test_precision'].append(initial_precision)\n",
    "al_results['test_recall'].append(initial_recall)\n",
    "al_results['test_f1'].append(initial_f1)\n",
    "al_results['pool_size'].append(len(X_unlabeled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a465d74",
   "metadata": {},
   "source": [
    "## Active Learning Iteration\n",
    "- we do another 20 loop iterations\n",
    "- 1 loop per 1 data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning iterations\n",
    "for iteration in range(n_queries):\n",
    "    print(f\"\\nIteration {iteration + 1}/{n_queries}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check if we still have unlabeled samples\n",
    "    if len(X_unlabeled) == 0:\n",
    "        print(\"No more unlabeled samples!\")\n",
    "        break\n",
    "        \n",
    "    # Query the most uncertain sample\n",
    "    query_idx, query_instance = learner.query(X_unlabeled, n_instances=1)\n",
    "    \n",
    "    # Get model's prediction probability BEFORE teaching\n",
    "    pred_proba = learner.predict_proba(query_instance.reshape(1, -1))[0]\n",
    "    uncertainty = 1 - np.max(pred_proba)  # Uncertainty measure\n",
    "    match_probability = pred_proba[1] * 100  # Probability of match (class 1) as percentage\n",
    "    no_match_probability = pred_proba[0] * 100  # Probability of no match (class 0) as percentage\n",
    "    \n",
    "    # Get the true label for the queried sample (simulate human labeling)\n",
    "    true_label = y_unlabeled_true[query_idx[0]]\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # TRACK QUERY HISTORY\n",
    "    # ================================\n",
    "    \n",
    "    \n",
    "    # Teach the learner with the new labeled sample\n",
    "    learner.teach(query_instance, [true_label])\n",
    "    \n",
    "    # **** The reason why the var is changed all the time is it's delete as soon as the learner is taught\n",
    "    # Remove the queried sample from unlabeled pool\n",
    "    X_unlabeled = np.delete(X_unlabeled, query_idx[0], axis=0)\n",
    "    y_unlabeled_true = np.delete(y_unlabeled_true, query_idx[0], axis=0)\n",
    "    \n",
    "    # Evaluate current performance\n",
    "    y_pred_current = learner.predict(X_test_al)\n",
    "    current_accuracy = accuracy_score(y_test_al, y_pred_current)\n",
    "    current_precision = precision_score(y_test_al, y_pred_current, zero_division=0)\n",
    "    current_recall = recall_score(y_test_al, y_pred_current, zero_division=0)\n",
    "    current_f1 = f1_score(y_test_al, y_pred_current, zero_division=0)\n",
    "    \n",
    "    # Store results\n",
    "    current_labeled = len(X_initial) + iteration + 1\n",
    "    al_results['n_labeled'].append(current_labeled)\n",
    "    al_results['test_accuracy'].append(current_accuracy)\n",
    "    al_results['test_precision'].append(current_precision)\n",
    "    al_results['test_recall'].append(current_recall)\n",
    "    al_results['test_f1'].append(current_f1)\n",
    "    al_results['pool_size'].append(len(X_unlabeled))\n",
    "    \n",
    "    # Enhanced progress display with uncertainty and probabilities\n",
    "    print(f\"Labeled samples: {current_labeled}\")\n",
    "    print(f\"Pool remaining: {len(X_unlabeled)}\")\n",
    "    print(f\"Queried label: {true_label} {'(MATCH)' if true_label == 1 else '(NO MATCH)'}\")\n",
    "    print(f\"Model uncertainty: {uncertainty:.4f}\")\n",
    "    print(f\"Match probability: {match_probability:.2f}%\")\n",
    "    print(f\"No-match probability: {no_match_probability:.2f}%\")\n",
    "    print(f\"Current F1: {current_f1:.4f}\")\n",
    "\n",
    "print(\"\\nActive Learning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d435fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_unlabeled[query_idx]       # Feature vector\n",
    "label = y_unlabeled_true[query_idx]     # True label\n",
    "# pair = pairs_unlabeled[query_idx]\n",
    "print(f\"\\nQueried Feature Vector: {features}\")\n",
    "print(f\"Queried True Label: {label}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4811874",
   "metadata": {},
   "source": [
    "## Plot Graph and Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8deaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Plot Active Learning Results\n",
    "\n",
    "# Start a new figure and set its overall size\n",
    "plt.figure(figsize=(12, 8)) # As requested: 20 inches wide, 6 inches tall\n",
    "\n",
    "# Define the first subplot (1 row, 2 columns, this is the 1st plot)\n",
    "# plt.subplot(1, 2, 1) # This makes the 'All Metrics' plot the left one\n",
    "\n",
    "# Plot 2: All Metrics vs Number of Labeled Samples\n",
    "# Directly use plt.plot, plt.xlabel, etc., as plt.subplot sets the current active axes\n",
    "plt.plot(al_results['n_labeled'], al_results['test_precision'],\n",
    "                marker='o', label='Precision', linewidth=2, color='#FF7F0E') # Orange\n",
    "plt.plot(al_results['n_labeled'], al_results['test_recall'],\n",
    "                marker='s', label='Recall', linewidth=2, color='#1F77B4')    # Blue\n",
    "plt.plot(al_results['n_labeled'], al_results['test_f1'],\n",
    "                marker='^', label='F1-Score', linewidth=2, color='#2CA02C')  # Green\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Active Learning: All Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define the second subplot (1 row, 2 columns, this is the 2nd plot)\n",
    "# plt.subplot(1, 2, 2) # This makes the 'Accuracy' plot the right one\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot 3: Accuracy vs Number of Labeled Samples\n",
    "# Directly use plt.plot, plt.xlabel, etc., for this subplot\n",
    "plt.plot(al_results['n_labeled'], al_results['test_accuracy'],\n",
    "                marker='o', label='Accuracy', linewidth=2, markersize=6, color='purple') # Teal\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Active Learning Progress (Accuracy Metrics)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# Summary of Active Learning Results\n",
    "print(f\"\\nActive Learning Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Initial labeled samples: {al_results['n_labeled'][0]}\")\n",
    "print(f\"Final labeled samples: {al_results['n_labeled'][-1]}\")\n",
    "print(f\"Total queries made: {len(al_results['n_labeled']) - 1}\")\n",
    "\n",
    "print(f\"\\nPerformance Improvement:\")\n",
    "print(f\"Initial F1-Score: {al_results['test_f1'][0]:.4f}\")\n",
    "print(f\"Final F1-Score: {al_results['test_f1'][-1]:.4f}\")\n",
    "print(f\"F1 Improvement: {al_results['test_f1'][-1] - al_results['test_f1'][0]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"Precision: {al_results['test_precision'][-1]:.4f}\")\n",
    "print(f\"Recall: {al_results['test_recall'][-1]:.4f}\")\n",
    "print(f\"F1-Score: {al_results['test_f1'][-1]:.4f}\")\n",
    "print(f\"Accuracy: {al_results['test_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Create results dataframe for analysis\n",
    "al_results_df = pd.DataFrame(al_results)\n",
    "print(f\"\\nActive Learning Progress Table:\")\n",
    "print(al_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cdd65",
   "metadata": {},
   "source": [
    "### Plot graph (Further detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Summary of Active Learning Results\n",
    "print(f\"\\nActive Learning Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Initial labeled samples: {al_results['n_labeled'][0]}\")\n",
    "print(f\"Final labeled samples: {al_results['n_labeled'][-1]}\")\n",
    "print(f\"Total queries made: {len(al_results['n_labeled']) - 1}\")\n",
    "print(f\"\\nPerformance Improvement:\")\n",
    "print(f\"Initial F1-Score: {al_results['test_f1'][0]:.4f}\")\n",
    "print(f\"Final F1-Score: {al_results['test_f1'][-1]:.4f}\")\n",
    "print(f\"F1 Improvement: {al_results['test_f1'][-1] - al_results['test_f1'][0]:.4f}\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"Precision: {al_results['test_precision'][-1]:.4f}\")\n",
    "print(f\"Recall: {al_results['test_recall'][-1]:.4f}\")\n",
    "print(f\"F1-Score: {al_results['test_f1'][-1]:.4f}\")\n",
    "print(f\"Accuracy: {al_results['test_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# %%\n",
    "# CONFUSION MATRIX AND FINAL EVALUATION\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL EVALUATION WITH CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get final predictions\n",
    "y_pred_final = learner.predict(X_test_al)\n",
    "y_pred_proba_final = learner.predict_proba(X_test_al)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_al, y_pred_final)\n",
    "\n",
    "# %%\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['No Match', 'Match'],\n",
    "            yticklabels=['No Match', 'Match'])\n",
    "plt.title('Confusion Matrix (Final Prediction) - Birth-Death Record Linkage ' \\\n",
    "    '\\n by using Active Learning')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Detailed Confusion Matrix Analysis\n",
    "print(\"\\nDETAILED CONFUSION MATRIX ANALYSIS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "specificity = tn/(tn+fp) if (tn+fp) > 0 else 0\n",
    "sensitivity = tp/(tp+fn) if (tp+fn) > 0 else 0\n",
    "precision = tp/(tp+fp) if (tp+fp) > 0 else 0\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "f1 = 2*(precision*sensitivity)/(precision+sensitivity) if (precision+sensitivity) > 0 else 0\n",
    "\n",
    "print(f\"Sensitivity (Recall):   {sensitivity*100:6.2f}% - % of true matches found\")\n",
    "print(f\"Specificity:            {specificity*100:6.2f}% - % of true non-matches found\")\n",
    "print(f\"Precision:              {precision*100:6.2f}% - % of predicted matches that are correct\")\n",
    "print(f\"Accuracy:               {accuracy*100:6.2f}% - Overall correctness\")\n",
    "print(f\"F1-Score:               {f1*100:6.2f}% - Harmonic mean of precision and recall\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nCLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 45)\n",
    "report = classification_report(y_test_al, y_pred_final, \n",
    "                             target_names=['No Match', 'Match'], \n",
    "                             digits=4)\n",
    "print(report)\n",
    "\n",
    "# %%\n",
    "# Active Learning Efficiency Analysis\n",
    "print(f\"\\nACTIVE LEARNING EFFICIENCY ANALYSIS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "initial_f1 = al_results['test_f1'][0]\n",
    "final_f1 = al_results['test_f1'][-1]\n",
    "improvement = final_f1 - initial_f1\n",
    "samples_used = al_results['n_labeled'][-1]\n",
    "initial_samples = al_results['n_labeled'][0]\n",
    "additional_samples = samples_used - initial_samples\n",
    "\n",
    "\n",
    "print(f\"\\nRECORD LINKAGE INTERPRETATION:\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"â€¢ True Matches Found:     {tp}/{tp+fn} ({tp/(tp+fn)*100:.1f}% recall)\")\n",
    "print(f\"â€¢ False Links Created:    {fp} (linking different people)\")\n",
    "print(f\"â€¢ Missed True Matches:    {fn} (same person not linked)\")\n",
    "print(f\"â€¢ Correct Non-Matches:    {tn} (different people correctly separated)\")\n",
    "\n",
    "if fp > fn:\n",
    "    print(f\"â†’ Model tends to be more aggressive (creates some false links)\")\n",
    "elif fn > fp:\n",
    "    print(f\"â†’ Model tends to be more conservative (misses some true matches)\")\n",
    "else:\n",
    "    print(f\"â†’ Model has balanced error types\")\n",
    "\n",
    "print(f\"\\nActive Learning analysis completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83097d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
