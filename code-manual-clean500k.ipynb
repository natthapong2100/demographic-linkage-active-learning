{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e9f0b3",
   "metadata": {},
   "source": [
    "# Passive Learning (Manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff94638",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da889431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birth-Death Record Linkage System\n",
      "==================================================\n",
      "Loading datasets...\n",
      "Birth records: 1297360 rows, 39 columns\n",
      "Death records: 1297360 rows, 38 columns\n",
      "\n",
      "Birth Records Columns:\n",
      "['ID', 'family', 'marriage', \"child's forname(s)\", \"child's surname\", 'birth day', 'birth month', 'birth year', 'address', 'sex', \"father's forename\", \"father's surname\", \"father's occupation\", \"mother's forename\", \"mother's maiden surname\", \"mother's occupation\", \"day of parents' marriage\", \"month of parents' marriage\", \"year of parents' marriage\", \"place of parent's marriage\", 'illegit', 'notes', 'Death', 'CHILD_IDENTITY', 'MOTHER_IDENTITY', 'FATHER_IDENTITY', 'DEATH_RECORD_IDENTITY', 'PARENT_MARRIAGE_RECORD_IDENTITY', 'FATHER_BIRTH_RECORD_IDENTITY', 'MOTHER_BIRTH_RECORD_IDENTITY', 'MARRIAGE_RECORD_IDENTITY1', 'MARRIAGE_RECORD_IDENTITY2', 'MARRIAGE_RECORD_IDENTITY3', 'MARRIAGE_RECORD_IDENTITY4', 'MARRIAGE_RECORD_IDENTITY5', 'MARRIAGE_RECORD_IDENTITY6', 'MARRIAGE_RECORD_IDENTITY7', 'MARRIAGE_RECORD_IDENTITY8', 'IMMIGRANT_GENERATION']\n",
      "\n",
      "Death Records Columns:\n",
      "['ID', 'forename(s) of deceased', 'surname of deceased', 'occupation', 'marital status', 'sex', 'name of spouse', \"spouse's occ\", 'day', 'month', 'year', 'address', 'age at death', \"father's forename\", \"father's surname\", \"father's occupation\", 'if father deceased', \"mother's forename\", \"mother's maiden surname\", \"mother's occupation\", 'if mother deceased', 'death code A', 'death code B', 'death code C', 'notes1', 'Birth', 'mar', 'DECEASED_IDENTITY', 'MOTHER_IDENTITY', 'FATHER_IDENTITY', 'SPOUSE_IDENTITY', 'BIRTH_RECORD_IDENTITY', 'PARENT_MARRIAGE_RECORD_IDENTITY', 'FATHER_BIRTH_RECORD_IDENTITY', 'MOTHER_BIRTH_RECORD_IDENTITY', 'SPOUSE_MARRIAGE_RECORD_IDENTITY', 'SPOUSE_BIRTH_RECORD_IDENTITY', 'IMMIGRANT_GENERATION']\n",
      "\n",
      "Missing values in Birth Records:\n",
      "ID                        0\n",
      "family                    0\n",
      "marriage              24294\n",
      "child's forname(s)        0\n",
      "child's surname           0\n",
      "birth day                 0\n",
      "birth month               0\n",
      "birth year                0\n",
      "address               24294\n",
      "sex                       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Death Records:\n",
      "ID                              0\n",
      "forename(s) of deceased         0\n",
      "surname of deceased             0\n",
      "occupation                 501738\n",
      "marital status                  0\n",
      "sex                             0\n",
      "name of spouse             593915\n",
      "spouse's occ               757899\n",
      "day                             0\n",
      "month                           0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Birth Records: (1297360, 13)\n",
      "Cleaned Death Records: (1297360, 13)\n"
     ]
    }
   ],
   "source": [
    "# Birth-Death Record Linkage Analysis\n",
    "# A comprehensive record linkage system for linking birth and death records\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "# from jellyfish import jaro_winkler, damerau_levenshtein_distance\n",
    "from jellyfish import jaro_similarity, jaro_winkler_similarity, levenshtein_distance, damerau_levenshtein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Birth-Death Record Linkage System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Data Preprocessing\n",
    "\n",
    "\n",
    "# %%\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# clean data (500k)\n",
    "birth_df = pd.read_csv('linkage data/data_500k/clean/birth_records.csv')\n",
    "death_df = pd.read_csv('linkage data/data_500k/clean/death_records.csv')\n",
    "\n",
    "print(f\"Birth records: {birth_df.shape[0]} rows, {birth_df.shape[1]} columns\")\n",
    "print(f\"Death records: {death_df.shape[0]} rows, {death_df.shape[1]} columns\")\n",
    "\n",
    "# %%\n",
    "# Examine the structure of both datasets\n",
    "print(\"\\nBirth Records Columns:\")\n",
    "print(birth_df.columns.tolist())\n",
    "print(f\"\\nDeath Records Columns:\")\n",
    "print(death_df.columns.tolist())\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in Birth Records:\")\n",
    "print(birth_df.isnull().sum().head(10))\n",
    "print(f\"\\nMissing values in Death Records:\")\n",
    "print(death_df.isnull().sum().head(10))\n",
    "\n",
    "# %%\n",
    "# Data preprocessing - select relevant columns for linkage\n",
    "birth_columns = [\n",
    "    'ID', 'child\\'s forname(s)', 'child\\'s surname', 'sex', \n",
    "    'father\\'s forename', 'father\\'s surname', 'father\\'s occupation',\n",
    "    'mother\\'s forename', 'mother\\'s maiden surname', 'mother\\'s occupation',\n",
    "    'birth year', 'address', 'Death'\n",
    "]\n",
    "\n",
    "death_columns = [\n",
    "    'ID', 'forename(s) of deceased', 'surname of deceased', 'sex',\n",
    "    'father\\'s forename', 'father\\'s surname', 'father\\'s occupation',\n",
    "    'mother\\'s forename', 'mother\\'s maiden surname', 'mother\\'s occupation',\n",
    "    'year', 'address', 'age at death'\n",
    "]\n",
    "\n",
    "# Create clean datasets\n",
    "birth_clean = birth_df[birth_columns].copy()\n",
    "death_clean = death_df[death_columns].copy()\n",
    "\n",
    "# Rename columns for consistency\n",
    "birth_clean.columns = ['birth_id', 'forename', 'surname', 'sex', \n",
    "                       'father_forename', 'father_surname', 'father_occupation',\n",
    "                       'mother_forename', 'mother_surname', 'mother_occupation',\n",
    "                       'birth_year', 'address', 'death_link']\n",
    "\n",
    "death_clean.columns = ['death_id', 'forename', 'surname', 'sex',\n",
    "                       'father_forename', 'father_surname', 'father_occupation', \n",
    "                       'mother_forename', 'mother_surname', 'mother_occupation',\n",
    "                       'death_year', 'address', 'age_at_death']\n",
    "\n",
    "# Fill missing values with empty strings for string columns\n",
    "string_cols = ['forename', 'surname', 'sex', 'father_forename', 'father_surname', \n",
    "               'father_occupation', 'mother_forename', 'mother_surname', \n",
    "               'mother_occupation', 'address']\n",
    "\n",
    "for col in string_cols:\n",
    "    birth_clean[col] = birth_clean[col].fillna('').astype(str)\n",
    "    death_clean[col] = death_clean[col].fillna('').astype(str)\n",
    "\n",
    "print(f\"\\nCleaned Birth Records: {birth_clean.shape}\")\n",
    "print(f\"Cleaned Death Records: {death_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4a7b5",
   "metadata": {},
   "source": [
    "### Ground-truth function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0f957ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth created: 1297360 record pairs\n",
      "Positive matches: 1297360\n",
      "Negative matches: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # %%\n",
    "# # Create ground truth labels using Death column from birth records and ID from death records\n",
    "# def create_ground_truth(birth_df, death_df):\n",
    "#     \"\"\"Create ground truth labels for record pairs\"\"\"\n",
    "#     ground_truth = []\n",
    "    \n",
    "#     # Create a mapping of death IDs for quick lookup\n",
    "#     death_ids = set(death_df['death_id'].values)\n",
    "    \n",
    "#     for _, birth_row in birth_df.iterrows():\n",
    "#         birth_id = birth_row['birth_id']\n",
    "#         death_link = birth_row['death_link']\n",
    "        \n",
    "#         for _, death_row in death_df.iterrows():\n",
    "#             death_id = death_row['death_id']\n",
    "            \n",
    "#             # True match if death_link matches death_id\n",
    "#             if pd.notna(death_link) and int(death_link) == death_id: # connected the ground truth\n",
    "#                 label = 1  # Match\n",
    "#             else:\n",
    "#                 label = 0  # No match\n",
    "                \n",
    "#             ground_truth.append({\n",
    "#                 'birth_id': birth_id,\n",
    "#                 'death_id': death_id,\n",
    "#                 'label': label\n",
    "#             })\n",
    "    \n",
    "#     return pd.DataFrame(ground_truth)\n",
    "\n",
    "# ground_truth_df = create_ground_truth(birth_clean, death_clean)\n",
    "# print(f\"\\nGround truth created: {len(ground_truth_df)} record pairs\")\n",
    "# print(f\"Positive matches: {ground_truth_df['label'].sum()}\")\n",
    "# print(f\"Negative matches: {len(ground_truth_df) - ground_truth_df['label'].sum()}\")\n",
    "\n",
    "\n",
    "# # Sample both datasets to XX rows\n",
    "# data_size = 5000\n",
    "# print(f\"Sampling datasets to {data_size} rows each...\")\n",
    "# print(\"=\" * 40)\n",
    "\n",
    "# # Original sizes\n",
    "# print(f\"Original birth records: {len(birth_clean):,}\")\n",
    "# print(f\"Original death records: {len(death_clean):,}\")\n",
    "\n",
    "# # Sample birth records to XX rows\n",
    "# birth_sample_size = min(data_size, len(birth_clean))\n",
    "# birth_sampled = birth_clean.sample(n=birth_sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Sample death records to XX rows  \n",
    "# death_sample_size = min(data_size, len(death_clean))\n",
    "# death_sampled = death_clean.sample(n=death_sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# print(f\"Sampled birth records: {len(birth_sampled):,}\")\n",
    "# print(f\"Sampled death records: {len(death_sampled):,}\")\n",
    "# print(f\"Total pairs to create: {len(birth_sampled) * len(death_sampled):,}\")\n",
    "\n",
    "# # Use sampled data for ground truth\n",
    "# birth_clean = birth_sampled\n",
    "# death_clean = death_sampled\n",
    "\n",
    "# print(\"Sampling completed!\")\n",
    "# print()\n",
    "\n",
    "\n",
    "# # %%\n",
    "# # Create ground truth labels using Death column from birth records and ID from death records\n",
    "# def create_ground_truth(birth_df, death_df):\n",
    "#     \"\"\"Create ground truth labels for record pairs\"\"\"\n",
    "#     import time\n",
    "    \n",
    "#     print(\"Creating ground truth labels...\")\n",
    "#     print(\"=\" * 40)\n",
    "    \n",
    "#     ground_truth = []\n",
    "    \n",
    "#     # Get total counts for progress tracking\n",
    "#     total_birth_records = len(birth_df)\n",
    "#     total_death_records = len(death_df)\n",
    "#     total_pairs = total_birth_records * total_death_records\n",
    "    \n",
    "#     print(f\"Birth records: {total_birth_records}\")\n",
    "#     print(f\"Death records: {total_death_records}\")\n",
    "#     print(f\"Total pairs to process: {total_pairs:,}\")\n",
    "#     print()\n",
    "    \n",
    "#     # Create a mapping of death IDs for quick lookup\n",
    "#     death_ids = set(death_df['death_id'].values)\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     pairs_processed = 0\n",
    "#     matches_found = 0\n",
    "    \n",
    "#     for birth_idx, birth_row in birth_df.iterrows():\n",
    "#         birth_id = birth_row['birth_id']\n",
    "#         death_link = birth_row['death_link']\n",
    "        \n",
    "#         # Progress update every 100 birth records\n",
    "#         if (birth_idx + 1) % 100 == 0 or (birth_idx + 1) == total_birth_records:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             progress_pct = ((birth_idx + 1) / total_birth_records) * 100\n",
    "#             pairs_so_far = (birth_idx + 1) * total_death_records\n",
    "            \n",
    "#             print(f\"Progress: {birth_idx + 1:4d}/{total_birth_records} birth records \"\n",
    "#                   f\"({progress_pct:5.1f}%) | \"\n",
    "#                   f\"Pairs: {pairs_so_far:7,} | \"\n",
    "#                   f\"Matches: {matches_found:4d} | \"\n",
    "#                   f\"Time: {elapsed_time:6.1f}s\")\n",
    "        \n",
    "#         for death_idx, death_row in death_df.iterrows():\n",
    "#             death_id = death_row['death_id']\n",
    "            \n",
    "#             # True match if death_link matches death_id\n",
    "#             if pd.notna(death_link) and int(death_link) == death_id:\n",
    "#                 label = 1  # Match\n",
    "#                 matches_found += 1\n",
    "#             else:\n",
    "#                 label = 0  # No match\n",
    "                \n",
    "#             ground_truth.append({\n",
    "#                 'birth_id': birth_id,\n",
    "#                 'death_id': death_id,\n",
    "#                 'label': label\n",
    "#             })\n",
    "            \n",
    "#             pairs_processed += 1\n",
    "    \n",
    "#     total_time = time.time() - start_time\n",
    "#     print()\n",
    "#     print(\"Ground truth creation completed!\")\n",
    "#     print(f\"Total time: {total_time:.1f} seconds\")\n",
    "#     print(f\"Pairs per second: {pairs_processed/total_time:,.0f}\")\n",
    "#     print()\n",
    "    \n",
    "#     return pd.DataFrame(ground_truth)\n",
    "\n",
    "# # Create ground truth with progress tracking\n",
    "# ground_truth_df = create_ground_truth(birth_clean, death_clean)\n",
    "\n",
    "# print(f\"Ground truth created: {len(ground_truth_df):,} record pairs\")\n",
    "# print(f\"Positive matches: {ground_truth_df['label'].sum():,}\")\n",
    "# print(f\"Negative matches: {len(ground_truth_df) - ground_truth_df['label'].sum():,}\")\n",
    "# print(f\"Match ratio: {ground_truth_df['label'].sum()/len(ground_truth_df)*100:.4f}%\") # 4 min 38 sec\n",
    " \n",
    "def create_efficient_ground_truth(birth_df, death_df):\n",
    "    \"\"\"\n",
    "    Most efficient approach: directly create ground truth from death_link column\n",
    "    Only creates records for actual linked pairs, O(n) complexity\n",
    "    \"\"\"\n",
    "    ground_truth = []\n",
    "    \n",
    "    # Create death_id lookup for validation\n",
    "    valid_death_ids = set(death_df['death_id'].values)\n",
    "    \n",
    "    for _, birth_row in birth_df.iterrows():\n",
    "        birth_id = birth_row['birth_id']\n",
    "        death_link = birth_row['death_link']\n",
    "        \n",
    "        # Only process records with valid death links\n",
    "        if pd.notna(death_link):\n",
    "            death_id = int(death_link)\n",
    "            \n",
    "            # Verify the death_id exists in death records\n",
    "            if death_id in valid_death_ids:\n",
    "                ground_truth.append({\n",
    "                    'birth_id': birth_id,\n",
    "                    'death_id': death_id,\n",
    "                    'label': 1  # This is a confirmed match\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(ground_truth)\n",
    "\n",
    "\n",
    "ground_truth_df = create_efficient_ground_truth(birth_clean, death_clean)\n",
    "\n",
    "print(f\"\\nGround truth created: {len(ground_truth_df)} record pairs\")\n",
    "print(f\"Positive matches: {ground_truth_df['label'].sum()}\")\n",
    "print(f\"Negative matches: {len(ground_truth_df) - ground_truth_df['label'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21f8741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         birth_id  death_id  label\n",
      "0          389246    389246      1\n",
      "1          478124    478124      1\n",
      "2          478140    478140      1\n",
      "3          478170    478170      1\n",
      "4          478366    478366      1\n",
      "...           ...       ...    ...\n",
      "1297355   3445945   3445945      1\n",
      "1297356   3445947   3445947      1\n",
      "1297357   3445949   3445949      1\n",
      "1297358   3445951   3445951      1\n",
      "1297359   3445953   3445953      1\n",
      "\n",
      "[1297360 rows x 3 columns]\n",
      "         birth_id  death_id  label\n",
      "0          389246    389246      1\n",
      "1          478124    478124      1\n",
      "2          478140    478140      1\n",
      "3          478170    478170      1\n",
      "4          478366    478366      1\n",
      "...           ...       ...    ...\n",
      "1297355   3445945   3445945      1\n",
      "1297356   3445947   3445947      1\n",
      "1297357   3445949   3445949      1\n",
      "1297358   3445951   3445951      1\n",
      "1297359   3445953   3445953      1\n",
      "\n",
      "[1297360 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# ground_truth_df.loc[ground_truth_df['label'] == 1]\n",
    "print(ground_truth_df)\n",
    "print(ground_truth_df.loc[ground_truth_df['label'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6cec",
   "metadata": {},
   "source": [
    "## 2. Indexing (Blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ceaaa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_minhash_signature(record, columns):\n",
    "#     \"\"\"Create MinHash signature for a record\"\"\"\n",
    "#     minhash = MinHash()\n",
    "#     for col in columns:\n",
    "#         data = str(record[col]).lower()\n",
    "#         for token in data.split():\n",
    "#             minhash.update(token.encode('utf8'))\n",
    "#     return minhash\n",
    "\n",
    "# def blocking_with_minhash(birth_df, death_df, threshold):\n",
    "#     \"\"\"Create blocks using MinHash LSH\"\"\"\n",
    "#     print(f\"Creating blocks with threshold: {threshold}\")\n",
    "    \n",
    "#     # Columns to use for blocking\n",
    "#     blocking_cols = ['forename', 'surname', 'father_surname', 'mother_surname']\n",
    "    \n",
    "#     # Create MinHash signatures for all records\n",
    "#     birth_signatures = {}\n",
    "#     death_signatures = {}\n",
    "    \n",
    "#     print(\"Creating MinHash signatures for birth records...\")\n",
    "#     for idx, row in birth_df.iterrows():\n",
    "#         birth_signatures[f\"birth_{row['birth_id']}\"] = create_minhash_signature(row, blocking_cols)\n",
    "    \n",
    "#     print(\"Creating MinHash signatures for death records...\")\n",
    "#     for idx, row in death_df.iterrows():\n",
    "#         death_signatures[f\"death_{row['death_id']}\"] = create_minhash_signature(row, blocking_cols)\n",
    "    \n",
    "#     # Create LSH index\n",
    "#     lsh = MinHashLSH(threshold=threshold)\n",
    "    \n",
    "#     # Insert all signatures\n",
    "#     print(\"Inserting signatures into LSH index...\")\n",
    "#     for key, sig in birth_signatures.items():\n",
    "#         lsh.insert(key, sig)\n",
    "#     for key, sig in death_signatures.items():\n",
    "#         lsh.insert(key, sig)\n",
    "    \n",
    "#     # Generate candidate pairs and detailed blocking information\n",
    "#     candidate_pairs = []\n",
    "#     birth_blocks = {}  # Dictionary to store blocks for each birth record\n",
    "    \n",
    "#     print(\"Generating candidate pairs...\")\n",
    "#     for birth_key, birth_sig in birth_signatures.items():\n",
    "#         similar_records = lsh.query(birth_sig)\n",
    "#         birth_id = int(birth_key.split('_')[1])\n",
    "        \n",
    "#         # Find death records in the same block\n",
    "#         death_records_in_block = []\n",
    "#         for similar_key in similar_records:\n",
    "#             if similar_key.startswith('death_'):\n",
    "#                 death_id = int(similar_key.split('_')[1])\n",
    "#                 candidate_pairs.append((birth_id, death_id))\n",
    "#                 death_records_in_block.append(death_id)\n",
    "        \n",
    "#         # Store block information (just for statistical purposes)\n",
    "#         birth_blocks[birth_id] = {\n",
    "#             'birth_id': birth_id,\n",
    "#             'death_records': death_records_in_block,\n",
    "#             'total_similar_records': len(similar_records),\n",
    "#             'death_count': len(death_records_in_block)\n",
    "#         }\n",
    "    \n",
    "#     # Remove duplicates from candidate pairs\n",
    "#     candidate_pairs = list(set(candidate_pairs))\n",
    "    \n",
    "#     print(f\"Generated {len(candidate_pairs)} candidate pairs\") # for computation\n",
    "#     print(f\"Generated {len(birth_blocks)} birth blocks\") # for illustration, exploration\n",
    "    \n",
    "#     return candidate_pairs, birth_blocks\n",
    "\n",
    "# # Generate candidate pairs through blocking\n",
    "# candidate_pairs, birth_blocks = blocking_with_minhash(birth_clean, death_clean, threshold=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e7474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating blocks with threshold: 0.7\n",
      "Creating MinHash signatures for death records and building LSH index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x104297450>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/modal-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "def create_minhash_signature(record, columns):\n",
    "    \"\"\"Create MinHash signature for a record\"\"\"\n",
    "    minhash = MinHash()\n",
    "    for col in columns:\n",
    "        data = str(record[col]).lower()\n",
    "        for token in data.split():\n",
    "            minhash.update(token.encode('utf8'))\n",
    "    return minhash\n",
    "\n",
    "def efficient_blocking_with_minhash(birth_df, death_df, threshold):\n",
    "    \"\"\"\n",
    "    Efficient blocking using MinHash LSH - avoids quadratic complexity\n",
    "    Only death records are indexed, then birth records query against them\n",
    "    \"\"\"\n",
    "    print(f\"Creating blocks with threshold: {threshold}\")\n",
    "    \n",
    "    # Columns to use for blocking\n",
    "    blocking_cols = ['forename', 'surname', 'father_surname', 'mother_surname']\n",
    "    \n",
    "    # Create LSH index and populate ONLY with death records\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    death_signatures = {}\n",
    "    \n",
    "    print(\"Creating MinHash signatures for death records and building LSH index...\")\n",
    "    for idx, row in death_df.iterrows():\n",
    "        death_id = row['death_id']\n",
    "        death_key = f\"death_{death_id}\"\n",
    "        signature = create_minhash_signature(row, blocking_cols)\n",
    "        \n",
    "        # Store signature and insert into LSH\n",
    "        death_signatures[death_key] = signature\n",
    "        lsh.insert(death_key, signature)\n",
    "    \n",
    "    # Now query each birth record against the death-only LSH index\n",
    "    candidate_pairs = []\n",
    "    birth_blocks = {}\n",
    "    \n",
    "    print(\"Querying birth records against death LSH index...\")\n",
    "    for idx, birth_row in birth_df.iterrows():\n",
    "        birth_id = birth_row['birth_id']\n",
    "        \n",
    "        # Create signature for this birth record\n",
    "        birth_signature = create_minhash_signature(birth_row, blocking_cols)\n",
    "        \n",
    "        # Query LSH to find similar death records\n",
    "        similar_death_keys = lsh.query(birth_signature)\n",
    "        \n",
    "        # Extract death IDs and create candidate pairs\n",
    "        death_records_in_block = []\n",
    "        for death_key in similar_death_keys:\n",
    "            death_id = int(death_key.split('_')[1])\n",
    "            candidate_pairs.append((birth_id, death_id))\n",
    "            death_records_in_block.append(death_id)\n",
    "        \n",
    "        # Store block information\n",
    "        birth_blocks[birth_id] = {\n",
    "            'birth_id': birth_id,\n",
    "            'death_records': death_records_in_block,\n",
    "            'death_count': len(death_records_in_block)\n",
    "        }\n",
    "    \n",
    "    print(f\"Generated {len(candidate_pairs)} candidate pairs\")\n",
    "    print(f\"Generated {len(birth_blocks)} birth blocks\")\n",
    "    \n",
    "    return candidate_pairs, birth_blocks\n",
    "\n",
    "# Alternative: Even more efficient with batch processing\n",
    "# def batch_efficient_blocking(birth_df, death_df, threshold, batch_size=1000):\n",
    "#     \"\"\"\n",
    "#     Process birth records in batches to reduce memory usage\n",
    "#     Most efficient for very large datasets\n",
    "#     \"\"\"\n",
    "#     print(f\"Creating blocks with threshold: {threshold} (batch processing)\")\n",
    "    \n",
    "#     blocking_cols = ['forename', 'surname', 'father_surname', 'mother_surname']\n",
    "    \n",
    "#     # Build death-only LSH index once\n",
    "#     lsh = MinHashLSH(threshold=threshold)\n",
    "#     print(\"Building death records LSH index...\")\n",
    "    \n",
    "#     for idx, row in death_df.iterrows():\n",
    "#         death_id = row['death_id']\n",
    "#         death_key = f\"death_{death_id}\"\n",
    "#         signature = create_minhash_signature(row, blocking_cols)\n",
    "#         lsh.insert(death_key, signature)\n",
    "    \n",
    "#     # Process birth records in batches\n",
    "#     candidate_pairs = []\n",
    "#     birth_blocks = {}\n",
    "    \n",
    "#     print(\"Processing birth records in batches...\")\n",
    "#     for start_idx in range(0, len(birth_df), batch_size):\n",
    "#         end_idx = min(start_idx + batch_size, len(birth_df))\n",
    "#         batch = birth_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "#         print(f\"Processing batch {start_idx//batch_size + 1}/{(len(birth_df) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "#         for idx, birth_row in batch.iterrows():\n",
    "#             birth_id = birth_row['birth_id']\n",
    "#             birth_signature = create_minhash_signature(birth_row, blocking_cols)\n",
    "            \n",
    "#             # Query against death records\n",
    "#             similar_death_keys = lsh.query(birth_signature)\n",
    "            \n",
    "#             death_records_in_block = []\n",
    "#             for death_key in similar_death_keys:\n",
    "#                 death_id = int(death_key.split('_')[1])\n",
    "#                 candidate_pairs.append((birth_id, death_id))\n",
    "#                 death_records_in_block.append(death_id)\n",
    "            \n",
    "#             birth_blocks[birth_id] = {\n",
    "#                 'birth_id': birth_id,\n",
    "#                 'death_records': death_records_in_block,\n",
    "#                 'death_count': len(death_records_in_block)\n",
    "#             }\n",
    "    \n",
    "#     print(f\"Generated {len(candidate_pairs)} candidate pairs\")\n",
    "#     print(f\"Generated {len(birth_blocks)} birth blocks\")\n",
    "    \n",
    "#     return candidate_pairs, birth_blocks\n",
    "\n",
    "# Usage - choose one approach:\n",
    "\n",
    "# Standard efficient approach:\n",
    "candidate_pairs, birth_blocks = efficient_blocking_with_minhash(birth_clean, death_clean, threshold=0.7)\n",
    "\n",
    "# For very large datasets, use batch processing:\n",
    "# candidate_pairs, birth_blocks = batch_efficient_blocking(birth_clean, death_clean, threshold=0.7, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db422e8b",
   "metadata": {},
   "source": [
    "- checking the candidate pair that is same with the birth_blocks variable\n",
    "- the diff is only the data types\n",
    "- And, birth_blocks count the one that dont have the death (candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_candidate_pairs_by_ID(checking_id):\n",
    "    r = []\n",
    "    for i in range(len(candidate_pairs)):\n",
    "        if candidate_pairs[i][0] == checking_id:\n",
    "            r.append(candidate_pairs[i])\n",
    "    print(f\"\\nCandidate pairs for birth ID {checking_id}: {r}\")\n",
    "    print(f\"Total candidate pairs for birth ID {checking_id}: {len(r)}\")\n",
    "\n",
    "checking_candidate_pairs_by_ID(checking_id=1383518)\n",
    "checking_candidate_pairs_by_ID(checking_id=963232)\n",
    "checking_candidate_pairs_by_ID(checking_id=1276997)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110d6ce",
   "metadata": {},
   "source": [
    "### Blocking Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a58e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed blocking results\n",
    "print(\"\\nDetailed Blocking Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show first 15 birth records and their associated death records\n",
    "print(\"Sample Blocks (First 15 Birth Records):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "birth_ids_sample = list(birth_blocks.keys())[:15]\n",
    "for i, birth_id in enumerate(birth_ids_sample):\n",
    "    block_info = birth_blocks[birth_id]\n",
    "    print(f\"Block {i+1}:\")\n",
    "    print(f\"  Birth ID: {birth_id}\")\n",
    "    print(f\"  Associated Death Records: {block_info['death_records']}\")\n",
    "    print(f\"  Number of Death Records: {block_info['death_count']}\")\n",
    "    print(f\"  Total Similar Records: {block_info['total_similar_records']}\")\n",
    "    \n",
    "    # Show names for context\n",
    "    birth_record = birth_clean[birth_clean['birth_id'] == birth_id].iloc[0]\n",
    "    print(f\"  Birth Record: {birth_record['forename']} {birth_record['surname']}\")\n",
    "    \n",
    "    if block_info['death_records']:\n",
    "        print(f\"  Death Record(s):\")\n",
    "        for death_id in block_info['death_records']:\n",
    "            death_record = death_clean[death_clean['death_id'] == death_id]\n",
    "            if not death_record.empty:\n",
    "                death_record = death_record.iloc[0]\n",
    "                print(f\"    ID {death_id}: {death_record['forename']} {death_record['surname']}\")\n",
    "    else:\n",
    "        print(f\"  No death records in this block\")\n",
    "    print(\"---\")\n",
    "\n",
    "# %%\n",
    "# Statistical analysis of blocking results\n",
    "print(f\"\\nBlocking Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "death_counts = [block_info['death_count'] for block_info in birth_blocks.values()]\n",
    "birth_records_with_deaths = sum(1 for count in death_counts if count > 0)\n",
    "birth_records_without_deaths = sum(1 for count in death_counts if count == 0)\n",
    "\n",
    "print(f\"Total birth records: {len(birth_blocks)}\")\n",
    "print(f\"Birth records with death candidates: {birth_records_with_deaths}\")\n",
    "print(f\"Birth records without death candidates: {birth_records_without_deaths}\")\n",
    "print(f\"Average death candidates per birth record: {np.mean(death_counts):.2f}\")\n",
    "print(f\"Max death candidates for a single birth record: {max(death_counts)}\")\n",
    "\n",
    "# %%\n",
    "# Distribution of death record counts per birth record\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(death_counts, bins=range(max(death_counts)+2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Number of Death Records per Birth Record')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Death Candidates per Birth Record')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show birth records with most death candidates\n",
    "plt.subplot(1, 2, 2)\n",
    "top_candidates = sorted(birth_blocks.items(), key=lambda x: x[1]['death_count'], reverse=True)[:10]\n",
    "birth_ids = [str(item[0]) for item in top_candidates]\n",
    "death_counts_top = [item[1]['death_count'] for item in top_candidates]\n",
    "\n",
    "plt.bar(birth_ids, death_counts_top)\n",
    "plt.xlabel('Birth Record ID')\n",
    "plt.ylabel('Number of Death Candidates')\n",
    "plt.title('Top 10 Birth Records by Death Candidates')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Show birth records with multiple death candidates\n",
    "print(f\"\\nBirth Records with Multiple Death Candidates:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "multiple_candidates = {birth_id: info for birth_id, info in birth_blocks.items() \n",
    "                      if info['death_count'] > 1}\n",
    "\n",
    "if multiple_candidates:\n",
    "    print(f\"Found {len(multiple_candidates)} birth records with multiple death candidates:\")\n",
    "    \n",
    "    for birth_id, block_info in sorted(multiple_candidates.items(), \n",
    "                                      key=lambda x: x[1]['death_count'], reverse=True)[:10]:\n",
    "        birth_record = birth_clean[birth_clean['birth_id'] == birth_id].iloc[0]\n",
    "        print(f\"\\nBirth ID {birth_id}: {birth_record['forename']} {birth_record['surname']}\")\n",
    "        print(f\"  Death candidates ({block_info['death_count']}):\")\n",
    "        \n",
    "        for death_id in block_info['death_records']:\n",
    "            death_record = death_clean[death_clean['death_id'] == death_id]\n",
    "            if not death_record.empty:\n",
    "                death_record = death_record.iloc[0]\n",
    "                print(f\"    ID {death_id}: {death_record['forename']} {death_record['surname']} \"\n",
    "                      f\"(Age: {death_record['age_at_death']})\")\n",
    "else:\n",
    "    print(\"No birth records have multiple death candidates with this threshold.\")\n",
    "\n",
    "# %%\n",
    "# Summary table\n",
    "print(f\"\\nBlocking Summary Table:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "summary_data = []\n",
    "for count in range(max(death_counts) + 1):\n",
    "    freq = death_counts.count(count)\n",
    "    percentage = (freq / len(death_counts)) * 100\n",
    "    summary_data.append({\n",
    "        'Death_Candidates': count,\n",
    "        'Birth_Records': freq,\n",
    "        'Percentage': f\"{percentage:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e30ed",
   "metadata": {},
   "source": [
    "## 3. Comparing\n",
    "### Feature\n",
    "- from append()\n",
    "- these are the features in each columns\n",
    "1. jaro winkler\n",
    "2. damerau levenshtein\n",
    "3. jaccard similarity\n",
    "4. age consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fe25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Comparing (Feature Engineering)\n",
    "\n",
    "# %%\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Calculate Jaccard similarity between two strings\"\"\"\n",
    "    set1 = set(str1.lower().split())\n",
    "    set2 = set(str2.lower().split())\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 0\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def calculate_features(birth_record, death_record):\n",
    "    \"\"\"Calculate similarity features between two records\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # String comparison columns\n",
    "    string_cols = ['forename', 'surname', 'sex', 'father_forename', 'father_surname', \n",
    "                   'mother_forename', 'mother_surname', 'address']\n",
    "    \n",
    "    for col in string_cols:\n",
    "        birth_val = str(birth_record[col]).lower()\n",
    "        death_val = str(death_record[col]).lower()\n",
    "        \n",
    "        # Jaro-Winkler similarity\n",
    "        jw_sim = jaro_winkler_similarity(birth_val, death_val)\n",
    "        features.append(jw_sim)\n",
    "        # features.append({'col': col, 'jw_sim': jw_sim})\n",
    "        \n",
    "        # Damerau-Levenshtein distance (normalized)\n",
    "        dl_dist = damerau_levenshtein_distance(birth_val, death_val)\n",
    "        max_len = max(len(birth_val), len(death_val))\n",
    "        dl_sim = 1 - (dl_dist / max_len) if max_len > 0 else 1\n",
    "        features.append(dl_sim)\n",
    "        # features.append({'col': col, 'dl_sim': dl_sim})\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        jaccard_sim = jaccard_similarity(birth_val, death_val)\n",
    "        features.append(jaccard_sim)\n",
    "        # features.append({'col': col, 'jaccard_sim': jaccard_sim})\n",
    "    \n",
    "    # Age consistency (if available)\n",
    "    # if pd.notna(birth_record['birth_year']) and pd.notna(death_record['death_year']) and pd.notna(death_record['age_at_death']):\n",
    "    #     expected_birth_year = death_record['death_year'] - death_record['age_at_death']\n",
    "    #     age_diff = abs(birth_record['birth_year'] - expected_birth_year)\n",
    "    #     age_consistency = 1 / (1 + age_diff)  # Exponential decay\n",
    "    #     features.append(age_consistency)\n",
    "    #     # features.append({'col': 'age_consistency', 'value': age_consistency})\n",
    "    # else:\n",
    "    #     features.append(0.5)  # Neutral value when age info is missing\n",
    "    #     # features.append({'col': 'age_consistency', 'value': 0.5})\n",
    "\n",
    "    try:\n",
    "        # Convert to numeric, handling potential string inputs\n",
    "        birth_year = pd.to_numeric(birth_record['birth_year'], errors='coerce')\n",
    "        death_year = pd.to_numeric(death_record['death_year'], errors='coerce')  \n",
    "        age_at_death = pd.to_numeric(death_record['age_at_death'], errors='coerce')\n",
    "        \n",
    "        if pd.notna(birth_year) and pd.notna(death_year) and pd.notna(age_at_death):\n",
    "            expected_birth_year = death_year - age_at_death\n",
    "            age_diff = abs(birth_year - expected_birth_year)\n",
    "            age_consistency = 1 / (1 + age_diff)  # Exponential decay\n",
    "            # features.append({'col': 'age_consistency', 'value': age_consistency})\n",
    "            features.append(age_consistency)\n",
    "        else:\n",
    "            # features.append({'col': 'age_consistency', 'value': 0.5})\n",
    "            features.append(0.5)\n",
    "    except:\n",
    "        # Fallback if conversion fails\n",
    "        # features.append({'col': 'age_consistency', 'value': 0.5})\n",
    "        features.append(0.5)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create feature matrix\n",
    "print(\"Calculating features for candidate pairs...\")\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Create lookup dictionaries for faster access\n",
    "birth_lookup = {row['birth_id']: row for _, row in birth_clean.iterrows()} # death link\n",
    "death_lookup = {row['death_id']: row for _, row in death_clean.iterrows()}\n",
    "ground_truth_lookup = {(row['birth_id'], row['death_id']): row['label'] \n",
    "                      for _, row in ground_truth_df.iterrows()}\n",
    "\n",
    "for birth_id, death_id in candidate_pairs: # use candidate pairs only\n",
    "    if birth_id in birth_lookup and death_id in death_lookup:\n",
    "        birth_record = birth_lookup[birth_id]\n",
    "        death_record = death_lookup[death_id]\n",
    "        \n",
    "        features = calculate_features(birth_record, death_record)\n",
    "        X.append(features)\n",
    "        \n",
    "        # Get ground truth label\n",
    "        label = ground_truth_lookup.get((birth_id, death_id))\n",
    "        y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Positive class ratio: {np.sum(y)/len(y):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[0:20]\n",
    "X[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84529a",
   "metadata": {},
   "source": [
    "## 4. Classification\n",
    "### Cross Validation (5 folds)\n",
    "- with Metrics performance; Precision, Recall, F1\n",
    "- draw Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b04035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "## randomly select 100 candidate pairs for testing *****\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training positive ratio: {np.sum(y_train)/len(y_train):.4f}\")\n",
    "print(f\"Test positive ratio: {np.sum(y_test)/len(y_test):.4f}\")\n",
    "\n",
    "# %%\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# %%\n",
    "# Cross-Validation Analysis (on training data only)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "print(\"\\nPerforming Cross-Validation Analysis (Training Data Only)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# %%\n",
    "# Check if we have enough samples for cross-validation\n",
    "if len(np.unique(y_train)) < 2:\n",
    "    print(\"Warning: Only one class present in the training data. Cannot perform classification.\")\n",
    "    print(\"This might indicate:\")\n",
    "    print(\"1. Threshold too high (try lowering it)\")\n",
    "    print(\"2. Ground truth issues\")\n",
    "    print(\"3. No actual matches in the candidate pairs\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "else:\n",
    "    # Define custom scorers\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'f1': make_scorer(f1_score, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # %%\n",
    "    # Perform 5-fold stratified cross-validation on training data only\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"5-Fold Stratified Cross-Validation Results (Training Data Only):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Initialize classifier for CV\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation on training data only\n",
    "    cv_results = cross_validate(\n",
    "        rf_classifier, X_train_scaled, y_train, \n",
    "        cv=cv, scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # %%\n",
    "    # Display cross-validation results\n",
    "    metrics = ['precision', 'recall', 'f1']\n",
    "    for metric in metrics:\n",
    "        test_scores = cv_results[f'test_{metric}'] # test_precision, test_recall, test_f1\n",
    "        train_scores = cv_results[f'train_{metric}'] # train_precision, train_recall, train_f1\n",
    "        \n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  Validation:  {test_scores.mean():.4f} (+/- {test_scores.std() * 2:.4f})\")\n",
    "        print(f\"  Training:    {train_scores.mean():.4f} (+/- {train_scores.std() * 2:.4f})\")\n",
    "        print(f\"  CV Folds:    {test_scores}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2829f",
   "metadata": {},
   "source": [
    "### Fit the model and do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96609bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Train Random Forest classifier on training set\n",
    "print(f\"\\n\\nTraining Final Model on Training Set:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# %%\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test_scaled)[:, 1] # select all rows, 1 col\n",
    "\n",
    "print(y_pred)\n",
    "print(\"-\"* 40)\n",
    "count_of_ones = (y_pred == 1).sum()\n",
    "count_of_zeros = (y_pred == 0).sum()\n",
    "print(\"COunt of 0: \", count_of_zeros)\n",
    "print(\"COunt of 1: \", count_of_ones)\n",
    "print(\"-\"* 40)\n",
    "print(y_pred_proba)\n",
    "print(\"Classification completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b29730",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "- with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Evaluation\n",
    "\n",
    "# %%\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# %%\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Match', 'Match'],\n",
    "            yticklabels=['No Match', 'Match'])\n",
    "plt.title('Confusion Matrix - Birth-Death Record Linkage')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Feature importance analysis\n",
    "feature_names = []\n",
    "string_cols = ['forename', 'surname', 'sex', 'father_forename', 'father_surname', \n",
    "               'mother_forename', 'mother_surname', 'address']\n",
    "\n",
    "for col in string_cols:\n",
    "    feature_names.extend([f'{col}_jw', f'{col}_dl', f'{col}_jaccard'])\n",
    "feature_names.append('age_consistency')\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# %%\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Analyze prediction probabilities\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='No Match', color='red')\n",
    "# plt.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Match', color='blue')\n",
    "# plt.xlabel('Prediction Probability')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Prediction Probabilities')\n",
    "# plt.legend()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int) \n",
    "    # the cutoff point of pred_proba to know what probabilities should we say \"this is a match\"\n",
    "    if np.sum(y_pred_thresh) > 0:\n",
    "        precisions.append(precision_score(y_test, y_pred_thresh))\n",
    "        recalls.append(recall_score(y_test, y_pred_thresh))\n",
    "        f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "    else:\n",
    "        precisions.append(0)\n",
    "        recalls.append(0)\n",
    "        f1_scores.append(0)\n",
    "\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o')\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='s')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score', marker='^')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance vs Threshold')\n",
    "plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Summary statistics\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"5-Fold Stratified Cross-Validation Results:\")\n",
    "for metric in metrics:\n",
    "    test_scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.capitalize()}: {test_scores.mean():.4f} (+/- {test_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total record pairs evaluated: {len(candidate_pairs)}\")\n",
    "print(f\"Actual matches in test set: {np.sum(y_test)}\")\n",
    "print(f\"Predicted matches in test set: {np.sum(y_pred)}\")\n",
    "print(f\"True positives: {cm[1,1]}\")\n",
    "print(f\"False positives: {cm[0,1]}\")\n",
    "print(f\"False negatives: {cm[1,0]}\")\n",
    "print(f\"True negatives: {cm[0,0]}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Save results (optional)\n",
    "results_df = pd.DataFrame({\n",
    "    'birth_id': [candidate_pairs[i][0] for i in range(len(X_test))],\n",
    "    'death_id': [candidate_pairs[i][1] for i in range(len(X_test))],\n",
    "    'actual_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'prediction_probability': y_pred_proba\n",
    "})\n",
    "\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c7e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
